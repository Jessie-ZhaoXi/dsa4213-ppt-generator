{"data/Figure_1_0.png": " The Transformer - model architecture.", "data/Figure_2_0.png": "Scaled Dot-Product Attention.", "data/Figure_2_1.png": "Multi-Head Attention consists of several                            attention layers running in parallel."}