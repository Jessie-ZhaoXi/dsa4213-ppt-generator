{"data/Figure_1_0.png": "The Transformer: A Novel Model Architecture", "data/Figure_2_0.png": "Scaled Dot-Product Attention: Enhancing Computational Efficiency", "data/Figure_2_1.png": "Multi-Head Attention: Parallel Processing Innovation"}