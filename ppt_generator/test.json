{"data/Figure_1_0.png": "Transformer Architecture: A Paradigm Shift in Model Design", "data/Figure_2_0.png": "Scaled Dot-Product Attention: Efficient Dependency Mapping", "data/Figure_2_1.png": "Multi-Head Attention: Parallel Processing for Enhanced Resolution"}